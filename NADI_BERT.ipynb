{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NADI_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanikanjirangat/NLP_DI_NADI/blob/main/NADI_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp9-0chb1rEz"
      },
      "source": [
        "!pip install transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV0_hzHGrgpo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "le = LabelEncoder()\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng3Rx_iFQa8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8319bf14-39b1-4aef-b251-61a8e8e6a0e5"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdUuj4wRrKjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98544783-6463-490b-d5ed-d43f3f34e0ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZibXUeHkrI8q"
      },
      "source": [
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "input_dir=os.path.join(root_dir,'NADI')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AraBERT model** : For running the AraBERT model use below code"
      ],
      "metadata": {
        "id": "IuC1ekabGGKC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXfXZKDcimKT"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig,AutoModelForSequenceClassification\n",
        "class Model:\n",
        "    def __init__(self,path):\n",
        "        # self.args = args\n",
        "        self.path=path\n",
        "        self.MAX_LEN=128\n",
        "        # self.tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
        "        #bashar-talafha/multi-dialect-bert-base-arabic\n",
        "        num_labels=18\n",
        "        self.config = AutoConfig.from_pretrained('aubmindlab/bert-base-arabert',num_labels=num_labels)\n",
        "        # if not os.path.isdir(self.opath):\n",
        "        #     os.makedirs(self.opath)\n",
        " \n",
        "\n",
        "    # def format_time(elapsed):\n",
        "    #   elapsed_rounded = int(round((elapsed)))\n",
        "    #   return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "            \n",
        "    def extract_data(self,name,XY=None):\n",
        "        file =self.path+name\n",
        "        df = pd.read_csv(file, delimiter='\\t')\n",
        "        df.replace(np.nan,'NIL', inplace=True)\n",
        "        sentences= df[\"#2_content\"].values\n",
        "        labels = df[\"#3_label\"].values\n",
        "        return (sentences,labels)\n",
        "    def extract_data_test(self,name,XY=None):\n",
        "      file =self.path+name\n",
        "      df = pd.read_csv(file, delimiter='\\t')\n",
        "      df.replace(np.nan,'NIL', inplace=True)\n",
        "      sentences= df[\"#2_content\"].values\n",
        "      #labels = df[\"#3_label\"].values\n",
        "      return sentences\n",
        "    \n",
        "\n",
        "    def process_inputs(self,sentences,labels):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      # sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      # torch_idx = torch.tensor(sentence_idx)\n",
        "      tags_vals = list(labels)\n",
        "      le.fit(labels)\n",
        "      le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      labels=le.fit_transform(labels)\n",
        "      \n",
        "      print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs, labels = input_ids, labels\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      # validation_masks = torch.tensor(validation_masks).to(torch.int64)\n",
        "      # self.types=torch.tensor(types).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks, self.labels)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=32)\n",
        "\n",
        "      # return (self.inputs,self.labels,self.masks,self.types)\n",
        "    def process_dev_inputs(self,sentences,labels):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      # sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      # torch_idx = torch.tensor(sentence_idx)\n",
        "      tags_vals = list(labels)\n",
        "      le.fit(labels)\n",
        "      le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      labels=le.fit_transform(labels)\n",
        "      \n",
        "      print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs, labels = input_ids, labels\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      # validation_masks = torch.tensor(validation_masks).to(torch.int64)\n",
        "      # self.types=torch.tensor(types).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks, self.labels)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.validationdataloader = DataLoader(self.data, sampler=self.sampler, batch_size=32)\n",
        "\n",
        "    # def process_inputs_test(self,sentences,labels,act_ids,batch_size=1):\n",
        "    #   sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "    #   sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "    #   torch_idx = torch.tensor(sentence_idx)\n",
        "    #   tags_vals = list(labels)\n",
        "    #   le.fit(labels)\n",
        "    #   le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    #   labels=le.fit_transform(labels)\n",
        "      \n",
        "    #   print(le_name_mapping)\n",
        "    #   # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    #   input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "    #   # Pad our input tokens\n",
        "    #   input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "    #   attention_masks = []\n",
        "\n",
        "    #   # Create a mask of 1s for each token followed by 0s for padding\n",
        "    #   for seq in input_ids:\n",
        "    #     seq_mask= [float(i>0) for i in seq]\n",
        "    #     attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "    #   # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "    #   # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    #   inputs, labels = input_ids, labels\n",
        "    #   masks,_= attention_masks, input_ids\n",
        "    #   # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "    #   self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "    #   # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "    #   self.labels = torch.tensor(labels).to(torch.int64)\n",
        "    #   # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "    #   self.act_ids = torch.tensor(act_ids).to(torch.int64)\n",
        "    #   # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "    #   self.masks = torch.tensor(masks).to(torch.int64)\n",
        "    #   self.torch_idx = torch.tensor(sentence_idx).to(torch.int64)\n",
        "    #   self.data = TensorDataset(self.inputs,self.masks, self.labels,self.torch_idx,self.act_ids)\n",
        "    #   self.sampler = RandomSampler(self.data)\n",
        "    #   self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=batch_size)\n",
        "    def process_inputs_test(self,sentences,batch_size=1):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      torch_idx = torch.tensor(sentence_idx)\n",
        "      # tags_vals = list(labels)\n",
        "      # le.fit(labels)\n",
        "      # le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      # labels=le.fit_transform(labels)\n",
        "      \n",
        "      # print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs = input_ids\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      #self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      #self.act_ids = torch.tensor(act_ids).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      self.torch_idx = torch.tensor(sentence_idx).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=batch_size)\n",
        "\n",
        "    def train_save_load(self,train=1,retrain=0,label_smoothing = -1,XY=None):\n",
        "      \n",
        "      #WEIGHTS_NAME = \"GDI2018_calibrateTrain4.bin\"\n",
        "      WEIGHTS_NAME = \"NADI_AraBERTTestA.bin\"\n",
        "      # WEIGHTS_NAME = \"GDI2018_mbertTrain4.bin\"\n",
        "      # WEIGHTS_NAME='GDITrain4_Bert_base_cased1.bin'\n",
        "      # WEIGHTS_NAME = \"GDI2018_calibrateTrain4LS_01.bin\"#calibrated with LS 0.1\n",
        "      # WEIGHTS_NAME = \"GDI2018_calibrateTrain4LS.bin\"\n",
        "      # WEIGHTS_NAME = \"GDI_VBERT_cased.bin\"\n",
        "\n",
        "      OUTPUT_DIR = input_dir\n",
        "      output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
        "      if retrain!=1:\n",
        "\n",
        "        \n",
        "       \n",
        "        # self.model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabert',config=self.config)\n",
        "      else:\n",
        "        # self.model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=4)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabert',config=self.config)\n",
        "        state_dict = torch.load(output_model_file)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        WEIGHTS_NAME = \"NADI_AraBERT_inc.bin\"\n",
        "\n",
        "        OUTPUT_DIR = input_dir\n",
        "        output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
        "      self.model.cuda()\n",
        "      param_optimizer = list(self.model.named_parameters())\n",
        "      no_decay = ['bias', 'gamma', 'beta']\n",
        "      optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},{'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                                                                                                                                     'weight_decay_rate': 0.0}]\n",
        "      #optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)\n",
        "      optimizer = AdamW(optimizer_grouped_parameters,lr=5e-5)\n",
        "      #optimizer = AdamW(optimizer_grouped_parameters,lr=3e-5)\n",
        "      \n",
        "      \n",
        "      train_loss_set = []\n",
        "      out_train={}\n",
        "      batch_size=32\n",
        "      true=[]\n",
        "      logits_all=[]\n",
        "      output_dicts = []\n",
        "      batch_size=32\n",
        "      epochs = 4\n",
        "      import time\n",
        "      start_time = time.time()\n",
        "      if train==1:\n",
        "        for _ in trange(epochs, desc=\"Epoch\"):\n",
        "          # Trainin\n",
        "          # Set our model to training mode (as opposed to evaluation mode\n",
        "          self.model.train()\n",
        "          # Tracking variables\n",
        "          tr_loss = 0\n",
        "          nb_tr_examples, nb_tr_steps = 0, 0\n",
        "          # Train the data for one epoch\n",
        "          for step, batch in enumerate(self.dataloader):\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids,b_input_mask, b_labels = batch\n",
        "            # Clear out the gradients (by default they accumulate)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            # loss = model(b_input_ids, token_type_ids=b_types, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss,logits= self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            if label_smoothing == -1:\n",
        "              logits=logits\n",
        "            else:\n",
        "              criterion = LabelSmoothingLoss(label_smoothing)\n",
        "              loss=criterion(logits,b_labels)\n",
        "            \n",
        "\n",
        "\n",
        "       \n",
        "            train_loss_set.append(loss.item())    \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            # Update tracking variables\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += b_input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "          print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
        "        torch.save(self.model.state_dict(), output_model_file)\n",
        "        \n",
        "      else:\n",
        "        state_dict = torch.load(output_model_file)\n",
        "        self.model.load_state_dict(state_dict) \n",
        "      return output_dicts\n",
        "\n",
        "    def eval(self,label_smoothing = -1):\n",
        "      batch_size=32\n",
        "      eval_loss = 0\n",
        "      # Put model in evaluation mod\n",
        "      self.model.eval()\n",
        "      # Tracking variables \n",
        "      self.predictions , self.true_labels = [], []\n",
        "      output_dicts=[]\n",
        "     \n",
        "      \n",
        "      for batch in self.dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids,b_input_mask, b_labels = batch\n",
        "        # # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          if label_smoothing == -1:\n",
        "            logits=outputs[0]\n",
        "            loss=outputs[1]\n",
        "          else:\n",
        "            criterion = LabelSmoothingLoss(label_smoothing)\n",
        "            loss=criterion(logits,b_labels)\n",
        "          \n",
        "          eval_loss += loss.item()\n",
        "          self.dataloader.set_description(f'eval loss = {(eval_loss / i):.6f}')\n",
        "      return eval_loss / len(self.dataloader)\n",
        "    \n",
        "    def simple_test(self):\n",
        "      batch_size=32\n",
        "      # Put model in evaluation mod\n",
        "      self.model.eval()\n",
        "      # Tracking variables \n",
        "      self.predictions , self.true_labels = [], []\n",
        "      output_dicts=[]\n",
        "      \n",
        "      \n",
        "      for batch in self.dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids,b_input_mask= batch\n",
        "        # # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          logits=outputs[0]\n",
        "          for j in range(logits.size(0)):\n",
        "            probs = F.softmax(logits[j], -1)\n",
        "            output_dict = {\n",
        "                # 'index': batch_size * i + j,\n",
        "                #'true': b_labels[j].cpu().numpy().tolist(),\n",
        "                'pred': logits[j].argmax().item(),\n",
        "                'conf': probs.max().item(),\n",
        "                'logits': logits[j].cpu().numpy().tolist(),\n",
        "                'probs': probs.cpu().numpy().tolist(),\n",
        "            }\n",
        "            output_dicts.append(output_dict)\n",
        "      #y_true = [output_dict['true'] for output_dict in output_dicts]\n",
        "      y_pred = [output_dict['pred'] for output_dict in output_dicts]\n",
        "      #y_conf = [output_dict['conf'] for output_dict in output_dicts]\n",
        "\n",
        "      #accuracy = accuracy_score(y_true, y_pred) * 100.\n",
        "      #f1 = f1_score(y_true, y_pred, average='macro') * 100.\n",
        "      #confidence = np.mean(y_conf) * 100.\n",
        "\n",
        "      # results_dict = {\n",
        "      #     'accuracy': accuracy_score(y_true, y_pred) * 100.,\n",
        "      #     'macro-F1': f1_score(y_true, y_pred, average='macro') * 100.,\n",
        "      #     'confidence': np.mean(y_conf) * 100.,\n",
        "      # }\n",
        "      # print(results_dict)\n",
        "      # print(classification_report(y_true,y_pred))\n",
        "      # print(confusion_matrix(y_true,y_pred))\n",
        "      print(\"writing the results...\")\n",
        "      class_map={'algeria': 0, 'bahrain': 1, 'egypt': 2, 'iraq': 3, 'jordan': 4, 'ksa': 5, 'kuwait': 6, 'lebanon': 7, 'libya': 8, 'morocco': 9, 'oman': 10, 'palestine': 11, 'qatar': 12, 'sudan': 13, 'syria': 14, 'tunisia': 15, 'uae': 16, 'yemen': 17}\n",
        "      c={}\n",
        "      for i in class_map:\n",
        "        c[class_map[i]]=i\n",
        "      p=[]\n",
        "      #a=[]\n",
        "      for i,k in enumerate(y_pred):\n",
        "          p.append(c[k])\n",
        "          #a.append(c[y_true[i]])\n",
        "      path=input_dir+\"/official_results/testa_1.txt\"\n",
        "      #gold_path=input_dir+\"/results/AraBERTgold.txt\"\n",
        "      with open(path,\"w\") as f:\n",
        "        for r in p:\n",
        "          f.write(str(r))\n",
        "          f.write(\"\\n\")\n",
        "      # with open(gold_path,\"w\") as f:\n",
        "      #   for r in a:\n",
        "      #     f.write(str(r))\n",
        "      #     f.write(\"\\n\")\n",
        "      return output_dicts\n",
        "\n",
        "    \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-dialect-Arabic-BERT**:For running the Multi-dialect-Arabic-BERT model use below code:"
      ],
      "metadata": {
        "id": "g0Q_dwOJGR_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig,AutoModelForSequenceClassification\n",
        "class Model:\n",
        "    def __init__(self,path):\n",
        "        # self.args = args\n",
        "        self.path=path\n",
        "        self.MAX_LEN=128\n",
        "        # self.tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bashar-talafha/multi-dialect-bert-base-arabic')\n",
        "        #bashar-talafha/multi-dialect-bert-base-arabic\n",
        "        num_labels=18\n",
        "        self.config = AutoConfig.from_pretrained('bashar-talafha/multi-dialect-bert-base-arabic',num_labels=num_labels)\n",
        "        # if not os.path.isdir(self.opath):\n",
        "        #     os.makedirs(self.opath)\n",
        " \n",
        "\n",
        "    # def format_time(elapsed):\n",
        "    #   elapsed_rounded = int(round((elapsed)))\n",
        "    #   return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "            \n",
        "    def extract_data(self,name,XY=None):\n",
        "        file =self.path+name\n",
        "        df = pd.read_csv(file, delimiter='\\t')\n",
        "        df.replace(np.nan,'NIL', inplace=True)\n",
        "        sentences= df[\"#2_content\"].values\n",
        "        labels = df[\"#3_label\"].values\n",
        "        return (sentences,labels)\n",
        "    def extract_data_test(self,name,XY=None):\n",
        "      file =self.path+name\n",
        "      df = pd.read_csv(file, delimiter='\\t')\n",
        "      df.replace(np.nan,'NIL', inplace=True)\n",
        "      sentences= df[\"#2_content\"].values\n",
        "      #labels = df[\"#3_label\"].values\n",
        "      return sentences\n",
        "    \n",
        "\n",
        "    def process_inputs(self,sentences,labels):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      # sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      # torch_idx = torch.tensor(sentence_idx)\n",
        "      tags_vals = list(labels)\n",
        "      le.fit(labels)\n",
        "      le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      labels=le.fit_transform(labels)\n",
        "      \n",
        "      print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs, labels = input_ids, labels\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      # validation_masks = torch.tensor(validation_masks).to(torch.int64)\n",
        "      # self.types=torch.tensor(types).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks, self.labels)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=32)\n",
        "\n",
        "      # return (self.inputs,self.labels,self.masks,self.types)\n",
        "    def process_dev_inputs(self,sentences,labels):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      # sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      # torch_idx = torch.tensor(sentence_idx)\n",
        "      tags_vals = list(labels)\n",
        "      le.fit(labels)\n",
        "      le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      labels=le.fit_transform(labels)\n",
        "      \n",
        "      print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs, labels = input_ids, labels\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      # validation_masks = torch.tensor(validation_masks).to(torch.int64)\n",
        "      # self.types=torch.tensor(types).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks, self.labels)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.validationdataloader = DataLoader(self.data, sampler=self.sampler, batch_size=32)\n",
        "\n",
        "    # def process_inputs_test(self,sentences,labels,act_ids,batch_size=1):\n",
        "    #   sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "    #   sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "    #   torch_idx = torch.tensor(sentence_idx)\n",
        "    #   tags_vals = list(labels)\n",
        "    #   le.fit(labels)\n",
        "    #   le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    #   labels=le.fit_transform(labels)\n",
        "      \n",
        "    #   print(le_name_mapping)\n",
        "    #   # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    #   input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "    #   # Pad our input tokens\n",
        "    #   input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "    #   attention_masks = []\n",
        "\n",
        "    #   # Create a mask of 1s for each token followed by 0s for padding\n",
        "    #   for seq in input_ids:\n",
        "    #     seq_mask= [float(i>0) for i in seq]\n",
        "    #     attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "    #   # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "    #   # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    #   inputs, labels = input_ids, labels\n",
        "    #   masks,_= attention_masks, input_ids\n",
        "    #   # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "    #   self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "    #   # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "    #   self.labels = torch.tensor(labels).to(torch.int64)\n",
        "    #   # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "    #   self.act_ids = torch.tensor(act_ids).to(torch.int64)\n",
        "    #   # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "    #   self.masks = torch.tensor(masks).to(torch.int64)\n",
        "    #   self.torch_idx = torch.tensor(sentence_idx).to(torch.int64)\n",
        "    #   self.data = TensorDataset(self.inputs,self.masks, self.labels,self.torch_idx,self.act_ids)\n",
        "    #   self.sampler = RandomSampler(self.data)\n",
        "    #   self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=batch_size)\n",
        "    def process_inputs_test(self,sentences,batch_size=1):\n",
        "      sentences= [self.tokenizer.encode_plus(sent,add_special_tokens=True, max_length=self.MAX_LEN,truncation='longest_first') for i,sent in enumerate(sentences)]\n",
        "      sentence_idx = np.linspace(0,len(sentences), len(sentences),False)\n",
        "      torch_idx = torch.tensor(sentence_idx)\n",
        "      # tags_vals = list(labels)\n",
        "      # le.fit(labels)\n",
        "      # le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "      # labels=le.fit_transform(labels)\n",
        "      \n",
        "      # print(le_name_mapping)\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [inputs[\"input_ids\"] for inputs in sentences]\n",
        "\n",
        "      # Pad our input tokens\n",
        "      input_ids = pad_sequences(input_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask= [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "        \n",
        "      # token_type_ids=[inputs[\"token_type_ids\"] for inputs in sentences]\n",
        "      # token_type_ids=pad_sequences(token_type_ids, maxlen=self.MAX_LEN,truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      inputs = input_ids\n",
        "      masks,_= attention_masks, input_ids\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "      self.inputs = torch.tensor(inputs).to(torch.int64)\n",
        "      # validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "      #self.labels = torch.tensor(labels).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      #self.act_ids = torch.tensor(act_ids).to(torch.int64)\n",
        "      # validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "      self.masks = torch.tensor(masks).to(torch.int64)\n",
        "      self.torch_idx = torch.tensor(sentence_idx).to(torch.int64)\n",
        "      self.data = TensorDataset(self.inputs,self.masks)\n",
        "      self.sampler = RandomSampler(self.data)\n",
        "      self.dataloader = DataLoader(self.data, sampler=self.sampler, batch_size=batch_size)\n",
        "\n",
        "    def train_save_load(self,train=1,retrain=0,label_smoothing = -1,XY=None):\n",
        "      \n",
        "      #WEIGHTS_NAME = \"GDI2018_calibrateTrain4.bin\"\n",
        "      WEIGHTS_NAME = \"NADI_multidialectAraBERT.bin\"\n",
        "      # WEIGHTS_NAME = \"GDI2018_mbertTrain4.bin\"\n",
        "      # WEIGHTS_NAME='GDITrain4_Bert_base_cased1.bin'\n",
        "      # WEIGHTS_NAME = \"GDI2018_calibrateTrain4LS_01.bin\"#calibrated with LS 0.1\n",
        "      # WEIGHTS_NAME = \"GDI2018_calibrateTrain4LS.bin\"\n",
        "      # WEIGHTS_NAME = \"GDI_VBERT_cased.bin\"\n",
        "\n",
        "      OUTPUT_DIR = input_dir\n",
        "      output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
        "      if retrain!=1:\n",
        "\n",
        "        \n",
        "       \n",
        "        # self.model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained('bashar-talafha/multi-dialect-bert-base-arabic',config=self.config)\n",
        "      else:\n",
        "        # self.model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=4)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained('bashar-talafha/multi-dialect-bert-base-arabic',config=self.config)\n",
        "        state_dict = torch.load(output_model_file)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        WEIGHTS_NAME = \"NADI_AraBERTdialect_inc.bin\"\n",
        "\n",
        "        OUTPUT_DIR = input_dir\n",
        "        output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
        "      self.model.cuda()\n",
        "      param_optimizer = list(self.model.named_parameters())\n",
        "      no_decay = ['bias', 'gamma', 'beta']\n",
        "      optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},{'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                                                                                                                                                     'weight_decay_rate': 0.0}]\n",
        "      #optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)\n",
        "      optimizer = AdamW(optimizer_grouped_parameters,lr=5e-5)\n",
        "      #optimizer = AdamW(optimizer_grouped_parameters,lr=3e-5)\n",
        "      \n",
        "      \n",
        "      train_loss_set = []\n",
        "      out_train={}\n",
        "      batch_size=32\n",
        "      true=[]\n",
        "      logits_all=[]\n",
        "      output_dicts = []\n",
        "      batch_size=32\n",
        "      epochs = 4\n",
        "      import time\n",
        "      start_time = time.time()\n",
        "      if train==1:\n",
        "        for _ in trange(epochs, desc=\"Epoch\"):\n",
        "          # Trainin\n",
        "          # Set our model to training mode (as opposed to evaluation mode\n",
        "          self.model.train()\n",
        "          # Tracking variables\n",
        "          tr_loss = 0\n",
        "          nb_tr_examples, nb_tr_steps = 0, 0\n",
        "          # Train the data for one epoch\n",
        "          for step, batch in enumerate(self.dataloader):\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids,b_input_mask, b_labels = batch\n",
        "            # Clear out the gradients (by default they accumulate)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            # loss = model(b_input_ids, token_type_ids=b_types, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss,logits= self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            if label_smoothing == -1:\n",
        "              logits=logits\n",
        "            else:\n",
        "              criterion = LabelSmoothingLoss(label_smoothing)\n",
        "              loss=criterion(logits,b_labels)\n",
        "            \n",
        "\n",
        "\n",
        "       \n",
        "            train_loss_set.append(loss.item())    \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            # Update tracking variables\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += b_input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "          print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time)) \n",
        "        torch.save(self.model.state_dict(), output_model_file)\n",
        "        \n",
        "      else:\n",
        "        state_dict = torch.load(output_model_file)\n",
        "        self.model.load_state_dict(state_dict) \n",
        "      return output_dicts\n",
        "\n",
        "    def eval(self,label_smoothing = -1):\n",
        "      batch_size=32\n",
        "      eval_loss = 0\n",
        "      # Put model in evaluation mod\n",
        "      self.model.eval()\n",
        "      # Tracking variables \n",
        "      self.predictions , self.true_labels = [], []\n",
        "      output_dicts=[]\n",
        "     \n",
        "      \n",
        "      for batch in self.dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids,b_input_mask, b_labels = batch\n",
        "        # # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          if label_smoothing == -1:\n",
        "            logits=outputs[0]\n",
        "            loss=outputs[1]\n",
        "          else:\n",
        "            criterion = LabelSmoothingLoss(label_smoothing)\n",
        "            loss=criterion(logits,b_labels)\n",
        "          \n",
        "          eval_loss += loss.item()\n",
        "          self.dataloader.set_description(f'eval loss = {(eval_loss / i):.6f}')\n",
        "      return eval_loss / len(self.dataloader)\n",
        "    \n",
        "    # def simple_test(self):\n",
        "    #   batch_size=32\n",
        "    #   # Put model in evaluation mod\n",
        "    #   self.model.eval()\n",
        "    #   # Tracking variables \n",
        "    #   self.predictions , self.true_labels = [], []\n",
        "    #   output_dicts=[]\n",
        "      \n",
        "      \n",
        "    #   for batch in self.dataloader:\n",
        "    #     # Add batch to GPU\n",
        "    #     batch = tuple(t.to(device) for t in batch)\n",
        "    #     # Unpack the inputs from our dataloader\n",
        "    #     b_input_ids,b_input_mask,b_labels= batch\n",
        "    #     # # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    #     with torch.no_grad():\n",
        "    #       # Forward pass, calculate logit predictions\n",
        "    #       outputs = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    #       logits=outputs[0]\n",
        "    #       for j in range(logits.size(0)):\n",
        "    #         probs = F.softmax(logits[j], -1)\n",
        "    #         output_dict = {\n",
        "    #             # 'index': batch_size * i + j,\n",
        "    #             'true': b_labels[j].cpu().numpy().tolist(),\n",
        "    #             'pred': logits[j].argmax().item(),\n",
        "    #             'conf': probs.max().item(),\n",
        "    #             'logits': logits[j].cpu().numpy().tolist(),\n",
        "    #             'probs': probs.cpu().numpy().tolist(),\n",
        "    #         }\n",
        "    #         output_dicts.append(output_dict)\n",
        "    #   y_true = [output_dict['true'] for output_dict in output_dicts]\n",
        "    #   y_pred = [output_dict['pred'] for output_dict in output_dicts]\n",
        "    #   y_conf = [output_dict['conf'] for output_dict in output_dicts]\n",
        "\n",
        "    #   accuracy = accuracy_score(y_true, y_pred) * 100.\n",
        "    #   f1 = f1_score(y_true, y_pred, average='macro') * 100.\n",
        "    #   confidence = np.mean(y_conf) * 100.\n",
        "\n",
        "    #   results_dict = {\n",
        "    #       'accuracy': accuracy_score(y_true, y_pred) * 100.,\n",
        "    #       'macro-F1': f1_score(y_true, y_pred, average='macro') * 100.,\n",
        "    #       'confidence': np.mean(y_conf) * 100.,\n",
        "    #   }\n",
        "    #   print(results_dict)\n",
        "    #   print(classification_report(y_true,y_pred))\n",
        "    #   print(confusion_matrix(y_true,y_pred))\n",
        "    #   print(\"writing the results...\")\n",
        "    #   class_map={'algeria': 0, 'bahrain': 1, 'egypt': 2, 'iraq': 3, 'jordan': 4, 'ksa': 5, 'kuwait': 6, 'lebanon': 7, 'libya': 8, 'morocco': 9, 'oman': 10, 'palestine': 11, 'qatar': 12, 'sudan': 13, 'syria': 14, 'tunisia': 15, 'uae': 16, 'yemen': 17}\n",
        "    #   c={}\n",
        "    #   for i in class_map:\n",
        "    #     c[class_map[i]]=i\n",
        "    #   p=[]\n",
        "    #   #a=[]\n",
        "    #   for i,k in enumerate(y_pred):\n",
        "    #       p.append(c[k])\n",
        "    #       #a.append(c[y_true[i]])\n",
        "    #   path=input_dir+\"/results/NADI_dialect_BERT.txt\"\n",
        "    #   #gold_path=input_dir+\"/results/AraBERTgold.txt\"\n",
        "    #   with open(path,\"w\") as f:\n",
        "    #     for r in p:\n",
        "    #       f.write(str(r))\n",
        "    #       f.write(\"\\n\")\n",
        "    #   # with open(gold_path,\"w\") as f:\n",
        "    #   #   for r in a:\n",
        "    #   #     f.write(str(r))\n",
        "    #   #     f.write(\"\\n\")\n",
        "    #   return output_dicts\n",
        "\n",
        "    def simple_test(self):\n",
        "      batch_size=32\n",
        "      # Put model in evaluation mod\n",
        "      self.model.eval()\n",
        "      # Tracking variables \n",
        "      self.predictions , self.true_labels = [], []\n",
        "      output_dicts=[]\n",
        "      \n",
        "      \n",
        "      for batch in self.dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids,b_input_mask= batch\n",
        "        # # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          logits=outputs[0]\n",
        "          for j in range(logits.size(0)):\n",
        "            probs = F.softmax(logits[j], -1)\n",
        "            output_dict = {\n",
        "                # 'index': batch_size * i + j,\n",
        "                #'true': b_labels[j].cpu().numpy().tolist(),\n",
        "                'pred': logits[j].argmax().item(),\n",
        "                'conf': probs.max().item(),\n",
        "                'logits': logits[j].cpu().numpy().tolist(),\n",
        "                'probs': probs.cpu().numpy().tolist(),\n",
        "            }\n",
        "            output_dicts.append(output_dict)\n",
        "      #y_true = [output_dict['true'] for output_dict in output_dicts]\n",
        "      y_pred = [output_dict['pred'] for output_dict in output_dicts]\n",
        "      #y_conf = [output_dict['conf'] for output_dict in output_dicts]\n",
        "\n",
        "      #accuracy = accuracy_score(y_true, y_pred) * 100.\n",
        "      #f1 = f1_score(y_true, y_pred, average='macro') * 100.\n",
        "      #confidence = np.mean(y_conf) * 100.\n",
        "\n",
        "      # results_dict = {\n",
        "      #     'accuracy': accuracy_score(y_true, y_pred) * 100.,\n",
        "      #     'macro-F1': f1_score(y_true, y_pred, average='macro') * 100.,\n",
        "      #     'confidence': np.mean(y_conf) * 100.,\n",
        "      # }\n",
        "      # print(results_dict)\n",
        "      # print(classification_report(y_true,y_pred))\n",
        "      # print(confusion_matrix(y_true,y_pred))\n",
        "      print(\"writing the results...\")\n",
        "      class_map={'algeria': 0, 'bahrain': 1, 'egypt': 2, 'iraq': 3, 'jordan': 4, 'ksa': 5, 'kuwait': 6, 'lebanon': 7, 'libya': 8, 'morocco': 9, 'oman': 10, 'palestine': 11, 'qatar': 12, 'sudan': 13, 'syria': 14, 'tunisia': 15, 'uae': 16, 'yemen': 17}\n",
        "      c={}\n",
        "      for i in class_map:\n",
        "        c[class_map[i]]=i\n",
        "      p=[]\n",
        "      #a=[]\n",
        "      for i,k in enumerate(y_pred):\n",
        "          p.append(c[k])\n",
        "          #a.append(c[y_true[i]])\n",
        "      path=input_dir+\"/official_results/testa_1.txt\"\n",
        "      #gold_path=input_dir+\"/results/AraBERTgold.txt\"\n",
        "      with open(path,\"w\") as f:\n",
        "        for r in p:\n",
        "          f.write(str(r))\n",
        "          f.write(\"\\n\")\n",
        "      # with open(gold_path,\"w\") as f:\n",
        "      #   for r in a:\n",
        "      #     f.write(str(r))\n",
        "      #     f.write(\"\\n\")\n",
        "      return output_dicts\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "LRJA_6ha8yz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgH67E9_2hpI"
      },
      "source": [
        "path=input_dir\n",
        "m = Model(path)\n",
        "XY=0\n",
        "sentences_train,labels_train=m.extract_data('/NADI2022_Subtask1_TRAIN.tsv',XY=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaC5Kr_VM1kJ"
      },
      "source": [
        "#sentences_dev,labels_dev=m.extract_data('/NADI2022_Subtask1_DEV.tsv',XY=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmH7Wesg3LrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de2073e-4cf1-4576-8be2-e16be4ed0ef9"
      },
      "source": [
        "m.process_inputs(sentences_train,labels_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'algeria': 0, 'bahrain': 1, 'egypt': 2, 'iraq': 3, 'jordan': 4, 'ksa': 5, 'kuwait': 6, 'lebanon': 7, 'libya': 8, 'morocco': 9, 'oman': 10, 'palestine': 11, 'qatar': 12, 'sudan': 13, 'syria': 14, 'tunisia': 15, 'uae': 16, 'yemen': 17}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to train the model, set train=1 else train=0"
      ],
      "metadata": {
        "id": "zJHlQwx3G26N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Metk0dVHmySA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae159483-d995-4eff-edb7-51f8939a8075"
      },
      "source": [
        "out=m.train_save_load(train=1,retrain=0,label_smoothing = -1,XY=0)#considering dev as test set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bashar-talafha/multi-dialect-bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#act_out_dev=m.simple_test()"
      ],
      "metadata": {
        "id": "GYBIjfvo_IfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ9fgSNyy-Qp"
      },
      "source": [
        "sentences_test=m.extract_data_test('/NADI2022_Subtask1_TEST-A_Unlabeled.tsv',XY=0)\n",
        "m.process_inputs_test(sentences_test)\n",
        "act_out_test=m.simple_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please remember to change the result paths accordingly before running these lines of codes."
      ],
      "metadata": {
        "id": "ak4MvIcwHNnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test=m.extract_data_test('/NADI2022_Subtask1_TEST-B_Unlabeled.tsv',XY=0)\n",
        "m.process_inputs_test(sentences_test)\n",
        "act_out_test=m.simple_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-yeb0MhAMo0",
        "outputId": "d28f830b-d9b2-46de-fd65-cbab80fbbf62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "writing the results...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! python NADI2022-ST1-Scorer.py results/AraBERTgold.txt results/AraBERTTestA.txt -verbose"
      ],
      "metadata": {
        "id": "-toGui7YMHed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}